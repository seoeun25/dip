camus.job.name=DIP ${topic} Job

etl.destination.path=${destination-dir}
etl.execution.base.path=/user/ndap/camus/${topic}/exec
etl.execution.history.path=/user/ndap/camus/${topic}/exec/history
etl.execution.timezone=${timezone}
etl.keep.count.files=true

camus.message.encoder.class=com.seoeun.camus.etl.kafka.coders.StringMessageEncoder
camus.message.decoder.class.${topic}=com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder

kafka.message.coder.schema.registry.class=com.nexr.schemaregistry.AvroSchemaRegistry
etl.schema.registry.url=${schema-registry}

etl.partitioner.class=com.seoeun.camus.etl.kafka.partitioner.SimpleDailyPartitioner
etl.partitioner.class.${topic}=com.seoeun.camus.etl.kafka.partitioner.SimpleDailyPartitioner

etl.record.writer.provider.class=com.linkedin.camus.etl.kafka.common.AvroRecordWriterProvider

etl.fail.on.errors=true

# max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=${task-count}
# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=-1
# events with a timestamp older than this will be discarded.
kafka.max.historical.days=31
# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=-1
kafka.move.to.earliest.offset=false
kafka.max.pull.size=${pull-size}

# if whitelist has values, only whitelisted topic are pulled. Nothing on the blacklist is pulled
kafka.blacklist.topics=
kafka.whitelist.topics=${topic}
log4j.configuration=false

# Name of the client as seen by kafka
kafka.client.name=camus
# The Kafka brokers to connect to, format: kafka.brokers=host1:port,host2:port,host3:port
kafka.brokers=${broker}

#Stops the mapper from getting inundated with Decoder exceptions for the same topic
#Default value is set to 10
max.decoder.exceptions.to.print=5

#Controls the submitting of counts to Kafka
#Default value set to true
post.tracking.counts.to.kafka=true
monitoring.event.class=com.seoeun.camus.common.DipMonitoringEvent

# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
etl.run.tracking.post=true
kafka.monitor.tier=
etl.counts.path=${count-dir}/${topic}
kafka.monitor.time.granularity=10

#etl.hourly=hourly
etl.daily=daily

# Should we ignore events that cannot be decoded (exception thrown by MessageDecoder)?
# `false` will fail the job, `true` will silently drop the event.
etl.ignore.schema.errors=false

# configure output compression for deflate or snappy. Defaults to deflate
mapred.output.compress